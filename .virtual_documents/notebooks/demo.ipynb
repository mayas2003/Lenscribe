


# Import required libraries
import sys
import os
sys.path.append('..')

from src.models.vgg16_classifier import VGG16Classifier
from src.models.blip_processor import BLIPProcessor
from src.utils.image_processor import ImageProcessor
import matplotlib.pyplot as plt
from PIL import Image





# Initialize VGG16 classifier
vgg16 = VGG16Classifier()
print("VGG16 model loaded successfully!")

# Initialize BLIP processor
blip = BLIPProcessor()
print(f"BLIP model loaded successfully! {blip.get_device_info()}")





# Load an image (replace with your image path)
image_path = "C:\\Users\delma\OneDrive\Pictures\Screenshots 1\Screenshot 2025-08-08 201311.png"

# Load and display the image
image = ImageProcessor.load_image(image_path)
ImageProcessor.display_image(image, "Original Image")





# Classify the image using VGG16
predictions = vgg16.predict(image_path, top_k=5)

print("VGG16 Predictions:")
for i, (class_id, class_name, confidence) in enumerate(predictions, 1):
    print(f"{i}. {class_name}: {confidence:.4f}")





# Generate caption using BLIP
caption = blip.generate_caption(image_path)
print(f"Generated Caption: {caption}")

# Generate multiple captions
captions = blip.generate_multiple_captions(image_path, num_captions=3)
print("\nMultiple Captions:")
for i, cap in enumerate(captions, 1):
    print(f"{i}. {cap}")





# Ask questions about the image
questions = [
    "What is in this image?",
    "What colors are prominent?",
    "How many objects can you see?"
]

for question in questions:
    answer = blip.answer_question(image_path, question)
    print(f"Q: {question}")
    print(f"A: {answer}\n")





# Extract features using VGG16
features = vgg16.extract_features(image_path)
print(f"VGG16 Feature vector shape: {features.shape}")
print(f"Feature vector (first 10 values): {features[:10]}")









